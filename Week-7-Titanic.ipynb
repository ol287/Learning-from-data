{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "sns.set_theme()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 : Load the dataset “Titanic.xlsx” using the pandas function read_excel. Then explore the dataset and the variables. Try to understand the datatypes of each variable, number of null values and range of each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"titanic3.xls\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 : For this task use the columns pclass,sex,age,sibsp,parch,fare as your features and survived as your target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 :\tNow find the number of null values in the dataset and perform null value imputation. Perform 5 types of null imputation. (after completing this step, you should have 5 different versions of your data each using a different strategy to replace/remove missing values)\n",
    "* Replace the missing values in the fare and age columns with their global means respectively\n",
    "* Replace the missing values in the fare and age columns with an invalid value like -1\n",
    "* Replace the missing values in the fare and age columns with random values sampled from the fare and age columns respectively. (So, if the age column has n missing values, randomly sample n not null values from the age column and then replace the missing values with the sampled values)\n",
    "* Use the pclass column to decide on the missing values in the fare column and the sex column to decide on the missing values of the age columns\n",
    "* Just drop all the missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 : Once the null values have been replaced, we can visualize the data. Try to make some plots or graphics using pandas or matplotlib or seaborn to identify  \n",
    "\n",
    "* The relationship between age, survived and pclass \n",
    "* The relationship between fare, survived and pclass \n",
    "* The relationship between sex, survived and pclass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 : Once you have these plot and graphics you can now begin with the modelling of the data. Divide the dataset into a suitable train-test split and then fit a SVM classifier model  (SVM) from sklearn. For the first experiment use the parameter kernel = “linear”.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Once the model is trained then find the following metrics. \n",
    "\n",
    "* Model score  \n",
    "* Confusion matrix \n",
    "* Classification report using the sklearn library. This will give you various metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 : Repeat the above step for all your datasets (remember the 5 versions of your dataset you created in step 3). Try to observe the difference in results if any (if you do not notice any changes then try to find out the importance of that feature using the function provided [step 10 - in the document or PDF and Question 9 in the python notebook]. It may happen that the feature is not important to the model so different null value replacement methods have no effect on the performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8 Repeat your experiments for RBF kernel and observe the differences between accuracy and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\tFinally, for your SVM models an utility function is provided which plots the relative importances of the features provide to the model. Use that function to understand which features are more important to the model and how does this change for the SVM model with linear and RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for plotting feature importances (use this only for SVM with Linear Kernel)\n",
    "def f_importances(classifier, names):\n",
    "\n",
    "    coef = abs(classifier.coef_[0])\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "# Utility function for plotting feature importances (use this only for SVM with RBF Kernel)\n",
    "def f_importances_RBF(classifier,names,X_test,y_test):\n",
    "    features = np.array(names)\n",
    "    perm_importance = permutation_importance(classifier, X_test, y_test)\n",
    "    sorted_idx = perm_importance.importances_mean.argsort()\n",
    "    plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "    plt.xlabel(\"Permutation Importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
